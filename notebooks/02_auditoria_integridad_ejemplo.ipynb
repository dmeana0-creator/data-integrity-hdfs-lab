{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Informe de Integridad y Métricas del Clúster\n",
        "\n",
        "Este notebook procesa las evidencias generadas por los scripts de automatización (`30_fsck_data_audit.py` y `60_fsck_backup_audit.py`) y presenta el resumen de métricas de rendimiento.\n",
        "\n",
        "**Objetivos:**\n",
        "1. Parsear logs de `fsck` para `/data` y `/backup`.\n",
        "2. Generar CSVs resumen de integridad.\n",
        "3. Presentar tabla de tiempos de ingestión y replicación (R7).\n",
        "4. Conclusiones finales.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports y configuración\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "085323fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalamos librerías necesarias (solo hace falta ejecutarlo una vez)\n",
        "!pip install pandas --break-system-packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea596bba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Añadimos la ruta de usuario donde pip instaló pandas\n",
        "# (Basado en el log anterior que mostraba /opt/bd/.local)\n",
        "ruta_librerias = os.path.expanduser(\"~/.local/lib/python3.12/site-packages\")\n",
        "sys.path.append(ruta_librerias)\n",
        "\n",
        "print(f\"Ruta añadida: {ruta_librerias}\")\n",
        "print(\"Ahora intenta importar pandas en la siguiente celda.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# CONFIGURACIÓN DE RUTAS\n",
        "# Los scripts 30 y 60 dejan los txt en 'raw_audits' dentro de la carpeta notebooks\n",
        "AUDIT_DIR = Path('raw_audits') \n",
        "\n",
        "# Verificamos que existe el directorio\n",
        "if not AUDIT_DIR.exists():\n",
        "    print(f\"ATENCIÓN: No encuentro la carpeta {AUDIT_DIR}. Asegúrate de haber ejecutado los scripts 30 y 60.\")\n",
        "    AUDIT_DIR.mkdir(exist_ok=True)\n",
        "else:\n",
        "    print(f\"Directorio de auditorías encontrado: {AUDIT_DIR.resolve()}\")\n",
        "    print(\"Archivos disponibles:\", [f.name for f in AUDIT_DIR.glob('*.txt')])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Función de Parseo y Procesamiento\n",
        "\n",
        "Contabilizamos palabras clave típicas:\n",
        "- `CORRUPT`\n",
        "- `MISSING`\n",
        "- `Under replicated`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_fsck_report(text: str):\n",
        "    \"\"\"\n",
        "    Extrae los VALORES numéricos de las métricas clave del reporte fsck.\n",
        "    Busca patrones como 'Missing blocks: 0' y extrae el 0.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Función auxiliar para extraer y sumar números encontrados tras un patrón\n",
        "    def get_count(pattern):\n",
        "        # Buscamos el número (\\d+) que va después de la etiqueta y dos puntos\n",
        "        matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "        # Convertimos a entero y sumamos (por si aparece en varias secciones)\n",
        "        return sum(int(m) for m in matches) if matches else 0\n",
        "\n",
        "    return {\n",
        "        # Busca \"Corrupt blocks: X\"\n",
        "        'CORRUPT': get_count(r'Corrupt blocks:\\s+(\\d+)'),\n",
        "        \n",
        "        # Busca \"Missing blocks: X\"\n",
        "        'MISSING': get_count(r'Missing blocks:\\s+(\\d+)'),\n",
        "        \n",
        "        # Busca \"Under-replicated blocks: X\" (ojo al guion o espacio)\n",
        "        'UNDER_REPLICATED': get_count(r'Under[- ]replicated blocks:\\s+(\\d+)'),\n",
        "        \n",
        "        # Busca explícitamente la frase final de éxito\n",
        "        'HEALTHY': 1 if \"is HEALTHY\" in text else 0\n",
        "    }\n",
        "\n",
        "data_rows = []\n",
        "backup_rows = []\n",
        "\n",
        "# Iteramos sobre todos los archivos txt en la carpeta raw_audits\n",
        "for log_file in sorted(AUDIT_DIR.glob('*.txt')):\n",
        "    try:\n",
        "        text = log_file.read_text(encoding='utf-8', errors='ignore')\n",
        "        metrics = parse_fsck_report(text)\n",
        "        \n",
        "        # Extraemos la fecha del nombre del archivo (ej: fsck_data_2026-02-04.txt)\n",
        "        fecha = log_file.stem.split('_')[-1]\n",
        "        metrics['fecha'] = fecha\n",
        "        metrics['archivo'] = log_file.name\n",
        "\n",
        "        # Clasificamos si es de /data o de /backup\n",
        "        if 'fsck_data' in log_file.name:\n",
        "            data_rows.append(metrics)\n",
        "        elif 'fsck_backup' in log_file.name:\n",
        "            backup_rows.append(metrics)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error leyendo {log_file.name}: {e}\")\n",
        "\n",
        "# Creamos DataFrames\n",
        "df_data = pd.DataFrame(data_rows).sort_values('fecha') if data_rows else pd.DataFrame()\n",
        "df_backup = pd.DataFrame(backup_rows).sort_values('fecha') if backup_rows else pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Resultados /data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# --- FUNCIÓN AUXILIAR ---\n",
        "def ejecutar_cmd(comando):\n",
        "    # Ejecuta el comando directamente en el sistema\n",
        "    print(f\"Ejecutando: {comando}\")\n",
        "    subprocess.run(comando, shell=True, check=True)\n",
        "\n",
        "print(\"Resultados para /data:\")\n",
        "display(df_data) # Solo funciona en Jupyter\n",
        "\n",
        "if not df_backup.empty:\n",
        "    # --- 1. PREPARACIÓN ---\n",
        "    # Calculamos la fecha primero para usarla en el nombre\n",
        "    dt_hoy = datetime.now().strftime('%Y-%m-%d')\n",
        "    \n",
        "    # Creamos el nombre con la fecha: \"resumen_auditoria_backup_2026-02-05.csv\"\n",
        "    nombre_archivo = f'resumen_auditoria_data_{dt_hoy}.csv'\n",
        "    \n",
        "    # Definimos la ruta local completa\n",
        "    ruta_csv_data = AUDIT_DIR / nombre_archivo\n",
        "\n",
        "    # --- 2. GUARDADO LOCAL ---\n",
        "    df_data.to_csv(ruta_csv_data, index=False)\n",
        "    print(f\"CSV generado en disco local: {ruta_csv_data}\")\n",
        "\n",
        "    # --- 3. SUBIDA A HDFS ---\n",
        "    try:\n",
        "        # Ruta carpeta destino\n",
        "        ruta_hdfs_dir = f\"/audit/fsck/dt={dt_hoy}\"\n",
        "        \n",
        "        # Ruta absoluta del archivo local que acabamos de crear\n",
        "        ruta_local_absoluta = ruta_csv_data.resolve()\n",
        "        \n",
        "        print(f\"Subiendo a HDFS: {ruta_hdfs_dir} ...\")\n",
        "\n",
        "        # PASO A: Subir archivo\n",
        "        # Al poner la carpeta destino con barra al final, el archivo conserva su nombre (con fecha)\n",
        "        ejecutar_cmd(f'hdfs dfs -put -f \"{ruta_local_absoluta}\" {ruta_hdfs_dir}/')\n",
        "\n",
        "        print(f\"Éxito: Archivo disponible en HDFS: {ruta_hdfs_dir}/{nombre_archivo}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error al ejecutar comando HDFS: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error general: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b269bff6",
      "metadata": {},
      "source": [
        "## 4) Resultados /backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f7c017",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# --- FUNCIÓN AUXILIAR ---\n",
        "def ejecutar_cmd(comando):\n",
        "    # Ejecuta el comando directamente en el sistema\n",
        "    print(f\"Ejecutando: {comando}\")\n",
        "    subprocess.run(comando, shell=True, check=True)\n",
        "\n",
        "print(\"Resultados para /backup:\")\n",
        "display(df_backup) # Solo funciona en Jupyter\n",
        "\n",
        "if not df_backup.empty:\n",
        "    # --- 1. PREPARACIÓN (Cambio Clave) ---\n",
        "    # Calculamos la fecha primero para usarla en el nombre\n",
        "    dt_hoy = datetime.now().strftime('%Y-%m-%d')\n",
        "    \n",
        "    # Creamos el nombre con la fecha: \"resumen_auditoria_backup_2026-02-05.csv\"\n",
        "    nombre_archivo = f'resumen_auditoria_backup_{dt_hoy}.csv'\n",
        "    \n",
        "    # Definimos la ruta local completa\n",
        "    ruta_csv_backup = AUDIT_DIR / nombre_archivo\n",
        "\n",
        "    # --- 2. GUARDADO LOCAL ---\n",
        "    df_backup.to_csv(ruta_csv_backup, index=False)\n",
        "    print(f\"CSV generado en disco local: {ruta_csv_backup}\")\n",
        "\n",
        "    # --- 3. SUBIDA A HDFS ---\n",
        "    try:\n",
        "        # Ruta carpeta destino\n",
        "        ruta_hdfs_dir = f\"/audit/fsck/dt={dt_hoy}\"\n",
        "        \n",
        "        # Ruta absoluta del archivo local que acabamos de crear\n",
        "        ruta_local_absoluta = ruta_csv_backup.resolve()\n",
        "        \n",
        "        print(f\"Subiendo a HDFS: {ruta_hdfs_dir} ...\")\n",
        "\n",
        "        # PASO A: Subir archivo\n",
        "        # Al poner la carpeta destino con barra al final, el archivo conserva su nombre (con fecha)\n",
        "        ejecutar_cmd(f'hdfs dfs -put -f \"{ruta_local_absoluta}\" {ruta_hdfs_dir}/')\n",
        "\n",
        "        print(f\"Éxito: Archivo disponible en HDFS: {ruta_hdfs_dir}/{nombre_archivo}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error al ejecutar comando HDFS: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error general: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d30fd0",
      "metadata": {},
      "source": [
        "## 5) Métricas de Rendimiento\n",
        "\n",
        "### 5.1 Tabla de Tiempos\n",
        "| Actividad | Factor de replicación | Tiempo |\n",
        "| :--- | :--- | :--- |\n",
        "| **Ingestión Diaria (Script 20)** | Fr=1 | `00.00 s` |\n",
        "| **Backup a /backup (Script 40)** | Fr=1 | `00.00 s`|\n",
        "| **Ingestión Diaria (Script 20)** | Fr=2 | `00.00 s`|\n",
        "| **Backup a /backup (Script 40)** | Fr=2 | `00.00 s` |\n",
        "| **Ingestión Diaria (Script 20)** | Fr=3 | `28.34 s` |\n",
        "| **Backup a /backup (Script 40)** | Fr=3 | `10.86 s` |\n",
        "| **Ingestión Diaria (Script 20)** | Fr=4 | `00.00 s` |\n",
        "| **Backup a /backup (Script 40)** | Fr=4 | `00.00 s` |\n",
        "\n",
        "### 5.2 Evidencias de Uso de Recursos\n",
        "### 5.2.1 Uso de Recursos Proceso de Ingestión\n",
        "\n",
        "### 5.2.2 Uso de Recursos Proceso de Backup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9fed47b",
      "metadata": {},
      "source": [
        "## 6. Conclusiones y Recomendaciones\n",
        "\n",
        "### Conclusiones\n",
        "1. **Integridad:** El sistema HDFS ha demostrado capacidad de autorrecuperación. Los reportes `fsck` muestran estado `HEALTHY`.\n",
        "2. **Backup:** La estrategia de backup intra-cluster funciona correctamente, validada por inventario y auditoría independiente.\n",
        "3. **Rendimiento:** Se observa una correlación entre el factor de replicación y el tiempo de ingestión.\n",
        "\n",
        "### Recomendaciones\n",
        "* **Monitorización:** Implementar alertas automáticas ante eventos `MISSING`.\n",
        "* **Escalabilidad:** Añadir nodos extra si se supera el umbral de almacenamiento del 80%."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
