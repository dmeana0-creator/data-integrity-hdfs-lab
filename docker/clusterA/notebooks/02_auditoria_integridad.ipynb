{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b2fcdf-5a93-4fe1-87ad-9e4410037fdc",
   "metadata": {},
   "source": [
    "# Informe de Integridad y Métricas del Clúster\n",
    "\n",
    "Este notebook procesa las evidencias generadas por los scripts de automatización (`30_fsck_data_audit.py` y `60_fsck_backup_audit.py`) y presenta el resumen de métricas de rendimiento.\n",
    "\n",
    "**Objetivos:**\n",
    "1. Parsear logs de `fsck` para `/data` y `/backup`.\n",
    "2. Generar CSVs resumen de integridad.\n",
    "3. Presentar tabla de tiempos de ingestión y replicación (R7).\n",
    "4. Conclusiones finales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3c9b5-ca6d-4201-ac2b-503d849e2347",
   "metadata": {},
   "source": [
    "## 1) Imports y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce897c-a160-4bec-bd5a-49e241163258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m454.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: six, numpy, python-dateutil, pandas\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/opt/bd/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed numpy-2.4.2 pandas-3.0.0 python-dateutil-2.9.0.post0 six-1.17.0\n"
     ]
    }
   ],
   "source": [
    "# Instalamos librerías necesarias (solo hace falta ejecutarlo una vez)\n",
    "!pip install pandas --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadimos la ruta de usuario donde pip instaló pandas\n",
    "# (Basado en el log anterior que mostraba /opt/bd/.local)\n",
    "ruta_librerias = os.path.expanduser(\"~/.local/lib/python3.12/site-packages\")\n",
    "sys.path.append(ruta_librerias)\n",
    "\n",
    "print(f\"Ruta añadida: {ruta_librerias}\")\n",
    "print(\"Ahora intenta importar pandas en la siguiente celda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3305f6-9358-49ee-8341-eaec3966bcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de auditorías encontrado: /media/notebooks/raw_audits\n",
      "Archivos disponibles: ['fsck_backup_dt=2026-02-10.txt', 'fsck_data_dt=2026-02-10.txt']\n",
      "Directorio de resumen de auditorías encontrado: /media/notebooks/resumen_audits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIGURACIÓN DE RUTAS\n",
    "# Los scripts 30 y 60 dejan los txt en 'raw_audits' dentro de la carpeta notebooks\n",
    "AUDIT_DIR = Path(\"raw_audits\")\n",
    "\n",
    "RESUMEN_DIR = Path(\"resumen_audits\")\n",
    "\n",
    "# Verificamos que existe el directorio\n",
    "if not AUDIT_DIR.exists():\n",
    "    print(f\"ATENCIÓN: No encuentro la carpeta {AUDIT_DIR}. Asegúrate de haber ejecutado los scripts 30 y 60.\")\n",
    "    AUDIT_DIR.mkdir(exist_ok=True)\n",
    "else:\n",
    "    print(f\"Directorio de auditorías encontrado: {AUDIT_DIR.resolve()}\")\n",
    "    print(\"Archivos disponibles:\", [f.name for f in AUDIT_DIR.glob('*.txt')])\n",
    "\n",
    "if not RESUMEN_DIR.exists():\n",
    "    print(f\"ATENCIÓN: No encuentro la carpeta {RESUMEN_DIR}. Procediendo a su creación\")\n",
    "    RESUMEN_DIR.mkdir(exist_ok=True)\n",
    "else:\n",
    "    print(f\"Directorio de resumen de auditorías encontrado: {RESUMEN_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04810357-b0c6-4b09-bc1b-f3fb8c8b6cd6",
   "metadata": {},
   "source": [
    "## 2) Función de Parseo y Procesamiento\n",
    "\n",
    "Contabilizamos palabras clave típicas:\n",
    "- `CORRUPT`\n",
    "- `MISSING`\n",
    "- `Under replicated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79e8ddd-33b5-439a-83d0-31cf00477647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fsck_report(text: str):\n",
    "    \"\"\"\n",
    "    Extrae los VALORES numéricos de las métricas clave del reporte fsck.\n",
    "    Busca patrones como 'Missing blocks: 0' y extrae el 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Función auxiliar para extraer y sumar números encontrados tras un patrón\n",
    "    def get_count(pattern):\n",
    "        # Buscamos el número (\\d+) que va después de la etiqueta y dos puntos\n",
    "        matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "        # Convertimos a entero y sumamos (por si aparece en varias secciones)\n",
    "        return sum(int(m) for m in matches) if matches else 0\n",
    "\n",
    "    return {\n",
    "        # Busca \"Corrupt blocks: X\"\n",
    "        'CORRUPT': get_count(r'Corrupt blocks:\\s+(\\d+)'),\n",
    "        \n",
    "        # Busca \"Missing blocks: X\"\n",
    "        'MISSING': get_count(r'Missing blocks:\\s+(\\d+)'),\n",
    "        \n",
    "        # Busca \"Under-replicated blocks: X\" (ojo al guion o espacio)\n",
    "        'UNDER_REPLICATED': get_count(r'Under[- ]replicated blocks:\\s+(\\d+)'),\n",
    "        \n",
    "        # Busca explícitamente la frase final de éxito\n",
    "        'HEALTHY': 1 if \"is HEALTHY\" in text else 0\n",
    "    }\n",
    "\n",
    "data_rows = []\n",
    "backup_rows = []\n",
    "\n",
    "# Iteramos sobre todos los archivos txt en la carpeta raw_audits\n",
    "for log_file in sorted(AUDIT_DIR.glob('*.txt')):\n",
    "    try:\n",
    "        text = log_file.read_text(encoding='utf-8', errors='ignore')\n",
    "        metrics = parse_fsck_report(text)\n",
    "        \n",
    "        # Extraemos la fecha del nombre del archivo (ej: fsck_data_2026-02-04.txt)\n",
    "        fecha = log_file.stem.split('_')[-1]\n",
    "        metrics['fecha'] = fecha\n",
    "        metrics['archivo'] = log_file.name\n",
    "\n",
    "        # Clasificamos si es de /data o de /backup\n",
    "        if 'fsck_data' in log_file.name:\n",
    "            data_rows.append(metrics)\n",
    "        elif 'fsck_backup' in log_file.name:\n",
    "            backup_rows.append(metrics)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo {log_file.name}: {e}\")\n",
    "\n",
    "# Creamos DataFrames\n",
    "df_data = pd.DataFrame(data_rows).sort_values('fecha') if data_rows else pd.DataFrame()\n",
    "df_backup = pd.DataFrame(backup_rows).sort_values('fecha') if backup_rows else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5024bd2-d18c-463d-b75f-1d1e1de2c3c5",
   "metadata": {},
   "source": [
    "## 3) Resultados /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203006f2-55b9-481d-906b-4824104481c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para /data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORRUPT</th>\n",
       "      <th>MISSING</th>\n",
       "      <th>UNDER_REPLICATED</th>\n",
       "      <th>HEALTHY</th>\n",
       "      <th>fecha</th>\n",
       "      <th>archivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>dt=2026-02-10</td>\n",
       "      <td>fsck_data_dt=2026-02-10.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CORRUPT  MISSING  UNDER_REPLICATED  HEALTHY          fecha  \\\n",
       "0        0        0                 0        1  dt=2026-02-10   \n",
       "\n",
       "                       archivo  \n",
       "0  fsck_data_dt=2026-02-10.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV generado en disco local: resumen_audits/resumen_auditoria_data_2026-02-10.csv\n",
      "Subiendo a HDFS: /audit/fsck/dt=2026-02-10 ...\n",
      "Ejecutando: hdfs dfs -put -f \"/media/notebooks/resumen_audits/resumen_auditoria_data_2026-02-10.csv\" /audit/fsck/dt=2026-02-10/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 20:07:37,724 WARN hdfs.DataStreamer: Exception in createBlockOutputStream blk_1073741859_1035\n",
      "java.io.IOException: Got error, status=ERROR, status message , ack with firstBadLink as 172.18.0.7:9866\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:128)\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:104)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1827)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1728)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)\n",
      "2026-02-10 20:07:37,726 WARN hdfs.DataStreamer: Abandoning BP-610021310-172.18.0.3-1770753826894:blk_1073741859_1035\n",
      "2026-02-10 20:07:37,766 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[172.18.0.7:9866,DS-877c5857-6151-437f-80e8-d0d244412889,DISK]\n",
      "2026-02-10 20:07:52,056 WARN hdfs.DataStreamer: Exception in createBlockOutputStream blk_1073741860_1036\n",
      "java.io.IOException: Got error, status=ERROR, status message , ack with firstBadLink as 172.18.0.4:9866\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:128)\n",
      "\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:104)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1827)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1728)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)\n",
      "2026-02-10 20:07:52,056 WARN hdfs.DataStreamer: Abandoning BP-610021310-172.18.0.3-1770753826894:blk_1073741860_1036\n",
      "2026-02-10 20:07:52,094 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[172.18.0.4:9866,DS-d47403eb-916b-4326-a870-3b45373fd0f5,DISK]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Éxito: Archivo disponible en HDFS: /audit/fsck/dt=2026-02-10/resumen_auditoria_data_2026-02-10.csv\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# --- FUNCIÓN AUXILIAR ---\n",
    "def ejecutar_cmd(comando):\n",
    "    # Ejecuta el comando directamente en el sistema\n",
    "    print(f\"Ejecutando: {comando}\")\n",
    "    subprocess.run(comando, shell=True, check=True)\n",
    "\n",
    "print(\"Resultados para /data:\")\n",
    "display(df_data)\n",
    "\n",
    "if not df_backup.empty:\n",
    "    # --- 1. PREPARACIÓN ---\n",
    "    # Calculamos la fecha primero para usarla en el nombre\n",
    "    dt_hoy = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Creamos el nombre con la fecha: \"resumen_auditoria_backup_2026-02-05.csv\"\n",
    "    nombre_archivo = f'resumen_auditoria_data_{dt_hoy}.csv'\n",
    "    \n",
    "    # Definimos la ruta local completa\n",
    "    ruta_csv_data = RESUMEN_DIR / nombre_archivo\n",
    "\n",
    "    # --- 2. GUARDADO LOCAL ---\n",
    "    df_data.to_csv(ruta_csv_data, index=False)\n",
    "    print(f\"CSV generado en disco local: {ruta_csv_data}\")\n",
    "\n",
    "    # --- 3. SUBIDA A HDFS ---\n",
    "    try:\n",
    "        # Ruta carpeta destino\n",
    "        ruta_hdfs_dir = f\"/audit/fsck/dt={dt_hoy}\"\n",
    "        \n",
    "        # Ruta absoluta del archivo local que acabamos de crear\n",
    "        ruta_local_absoluta = ruta_csv_data.resolve()\n",
    "        \n",
    "        print(f\"Subiendo a HDFS: {ruta_hdfs_dir} ...\")\n",
    "\n",
    "        # PASO A: Subir archivo\n",
    "        # Al poner la carpeta destino con barra al final, el archivo conserva su nombre (con fecha)\n",
    "        ejecutar_cmd(f'hdfs dfs -put -f \"{ruta_local_absoluta}\" {ruta_hdfs_dir}/')\n",
    "\n",
    "        print(f\"Éxito: Archivo disponible en HDFS: {ruta_hdfs_dir}/{nombre_archivo}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error al ejecutar comando HDFS: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error general: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a6813-6af7-449a-8871-404321ebc20f",
   "metadata": {},
   "source": [
    "## 4) Resultados /backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b363147f-ccba-4e19-b2a3-4a62e61fbe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para /backup:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORRUPT</th>\n",
       "      <th>MISSING</th>\n",
       "      <th>UNDER_REPLICATED</th>\n",
       "      <th>HEALTHY</th>\n",
       "      <th>fecha</th>\n",
       "      <th>archivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>dt=2026-02-10</td>\n",
       "      <td>fsck_backup_dt=2026-02-10.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CORRUPT  MISSING  UNDER_REPLICATED  HEALTHY          fecha  \\\n",
       "0        0        0                 0        1  dt=2026-02-10   \n",
       "\n",
       "                         archivo  \n",
       "0  fsck_backup_dt=2026-02-10.txt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV generado en disco local: raw_audits/resumen_auditoria_backup_2026-02-10.csv\n",
      "Subiendo a HDFS: /audit/fsck/dt=2026-02-10 ...\n",
      "Ejecutando: hdfs dfs -put -f \"/media/notebooks/raw_audits/resumen_auditoria_backup_2026-02-10.csv\" /audit/fsck/dt=2026-02-10/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 20:07:57,144 WARN hdfs.DataStreamer: Exception in createBlockOutputStream blk_1073741862_1038\n",
      "java.net.NoRouteToHostException: No route to host\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:253)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1774)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1728)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)\n",
      "2026-02-10 20:07:57,146 WARN hdfs.DataStreamer: Abandoning BP-610021310-172.18.0.3-1770753826894:blk_1073741862_1038\n",
      "2026-02-10 20:07:57,193 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[172.18.0.7:9866,DS-877c5857-6151-437f-80e8-d0d244412889,DISK]\n",
      "2026-02-10 20:08:03,023 WARN hdfs.DataStreamer: Exception in createBlockOutputStream blk_1073741863_1039\n",
      "java.net.NoRouteToHostException: No route to host\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:253)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1774)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1728)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)\n",
      "2026-02-10 20:08:03,024 WARN hdfs.DataStreamer: Abandoning BP-610021310-172.18.0.3-1770753826894:blk_1073741863_1039\n",
      "2026-02-10 20:08:03,063 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[172.18.0.4:9866,DS-d47403eb-916b-4326-a870-3b45373fd0f5,DISK]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Éxito: Archivo disponible en HDFS: /audit/fsck/dt=2026-02-10/resumen_auditoria_backup_2026-02-10.csv\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# --- FUNCIÓN AUXILIAR ---\n",
    "def ejecutar_cmd(comando):\n",
    "    # Ejecuta el comando directamente en el sistema\n",
    "    print(f\"Ejecutando: {comando}\")\n",
    "    subprocess.run(comando, shell=True, check=True)\n",
    "\n",
    "print(\"Resultados para /backup:\")\n",
    "display(df_backup) # Solo funciona en Jupyter\n",
    "\n",
    "if not df_backup.empty:\n",
    "    # --- 1. PREPARACIÓN (Cambio Clave) ---\n",
    "    # Calculamos la fecha primero para usarla en el nombre\n",
    "    dt_hoy = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Creamos el nombre con la fecha: \"resumen_auditoria_backup_2026-02-05.csv\"\n",
    "    nombre_archivo = f'resumen_auditoria_backup_{dt_hoy}.csv'\n",
    "    \n",
    "    # Definimos la ruta local completa\n",
    "    ruta_csv_backup = AUDIT_DIR / nombre_archivo\n",
    "\n",
    "    # --- 2. GUARDADO LOCAL ---\n",
    "    df_backup.to_csv(ruta_csv_backup, index=False)\n",
    "    print(f\"CSV generado en disco local: {ruta_csv_backup}\")\n",
    "\n",
    "    # --- 3. SUBIDA A HDFS ---\n",
    "    try:\n",
    "        # Ruta carpeta destino\n",
    "        ruta_hdfs_dir = f\"/audit/fsck/dt={dt_hoy}\"\n",
    "        \n",
    "        # Ruta absoluta del archivo local que acabamos de crear\n",
    "        ruta_local_absoluta = ruta_csv_backup.resolve()\n",
    "        \n",
    "        print(f\"Subiendo a HDFS: {ruta_hdfs_dir} ...\")\n",
    "\n",
    "        # PASO A: Subir archivo\n",
    "        # Al poner la carpeta destino con barra al final, el archivo conserva su nombre (con fecha)\n",
    "        ejecutar_cmd(f'hdfs dfs -put -f \"{ruta_local_absoluta}\" {ruta_hdfs_dir}/')\n",
    "\n",
    "        print(f\"Éxito: Archivo disponible en HDFS: {ruta_hdfs_dir}/{nombre_archivo}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error al ejecutar comando HDFS: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error general: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086813f-6710-4004-8309-8b557475ef79",
   "metadata": {},
   "source": [
    "## 5) Métricas de Rendimiento\n",
    "\n",
    "### 5.1) Tabla de Tiempos\n",
    "| Actividad | Factor de replicación | Tiempo |\n",
    "| :--- | :--- | :--- |\n",
    "| **Ingesta de datos (Script 20)** | Block replication = 1 | `26.98 s` |\n",
    "| **Copia/Replicación (Script 40)** | Block replication = 1  | `10.62 s`|\n",
    "| **Ingesta de datos (Script 20)** | Block replication = 2 | `33.85 s`|\n",
    "| **Copia/Replicación (Script 40)** | Block replication = 2 | `11.01 s` |\n",
    "| **Ingesta de datos (Script 20)** | Block replication = 3 | `35.41 s` |\n",
    "| **Copia/Replicación (Script 40)** | Block replication = 3 | `14.55 s` |\n",
    "| **Ingesta de datos (Script 20)** | Block replication = 4 | `38.54 s` |\n",
    "| **Copia/Replicación (Script 40)** | Block replication = 4 | `16.17 s` |\n",
    "\n",
    "### 5.2) Evidencias de Uso de Recursos\n",
    "### 5.2.1) Uso de Recursos Proceso de Ingestión\n",
    "- Factor de replicación 1 :\n",
    "![Captura Subida a HDFS (Ingesta) con Factor de Replicación = 1](./img/stats_ingestion_fr1.png)\n",
    "\n",
    "- Factor de replicación 2 :\n",
    "![Captura Subida a HDFS (Ingesta) con Factor de Replicación = 2](./img/stats_ingestion_fr2.png)\n",
    "\n",
    "- Factor de replicación 3 :\n",
    "![Captura Subida a HDFS (Ingesta) con Factor de Replicación = 3](./img/stats_ingestion_fr3.png)\n",
    "\n",
    "- Factor de replicación 4 :\n",
    "![Captura Subida a HDFS (Ingesta) con Factor de Replicación = 4](./img/stats_ingestion_fr4.png)\n",
    "\n",
    "### 5.2.2) Uso de Recursos Proceso de Backup\n",
    "- Factor de replicación 1 :\n",
    "![Captura Replicación/copia (Backup) en HDFS con Factor de Replicación = 1](./img/stats_backup_fr1.png)\n",
    "\n",
    "- Factor de replicación 2 :\n",
    "![Captura Replicación/copia (Backup) en HDFS con Factor de Replicación = 2](./img/stats_backup_fr2.png)\n",
    "\n",
    "- Factor de replicación 3 :\n",
    "![Captura Replicación/copia (Backup) en HDFS con Factor de Replicación = 3](./img/stats_backup_fr3.png)\n",
    "\n",
    "- Factor de replicación 4 :\n",
    "![Captura Replicación/copia (Backup) en HDFS con Factor de Replicación = 4](./img/stats_backup_fr4.png)\n",
    "\n",
    "### 5.3) Análisis de Impacto en el Almacenamiento\n",
    "\n",
    "| Replicación | DFS Used (Aprox.) | Comportamiento | Impacto Operativo |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Factor 1** | `~1.00 GB` | **Sin redundancia.** 1 copia única por bloque. | **Crítico:** La caída de 1 nodo implica pérdida de datos inmediata. |\n",
    "| **Factor 2** | `~2.01 GB` | **Duplicación.** 2 copias por bloque. | **Bajo:** Soporta la caída de **1 nodo** sin interrumpir el servicio. |\n",
    "| **Factor 3** | `~3.01 GB` | **Triplicación (Default).** Estándar de Hadoop. | **Óptimo:** Balance coste/seguridad. Soporta la caída de **2 nodos**. |\n",
    "| **Factor 4** | `~4.01 GB` | **Copia Total.** Todos los nodos tienen todo. | **Máximo:** Seguridad total, pero con el coste de almacenamiento más alto (x4). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfebc5-d593-4cbc-ac6b-f35bb73a1738",
   "metadata": {},
   "source": [
    "## 6) Conclusiones y Recomendaciones\n",
    "A partir del análisis de las métricas de rendimiento y almacenamiento obtenidas en la simulación, se presentan las siguientes conclusiones técnicas y las recomendaciones arquitectónicas derivadas.\n",
    "\n",
    "### 6.1) Conclusiones\n",
    "- **Escalabilidad No Lineal en la Escritura:** El incremento del Factor de Replicación de 1 a 4 resultó en un aumento del tiempo de ingesta de solo un ~42%. Esto evidencia que el cuello de botella principal no es la replicación interna entre los nodos del clúster (que ocurre en paralelo), sino la transferencia de datos desde el cliente externo hacia la infraestructura.\n",
    "\n",
    "- **Eficiencia del Tráfico Intra-Clúster:** Las operaciones de respaldo interno (Backup) demostraron ser 2.5 veces más rápidas que la ingesta inicial. Este dato valida la eficiencia de la red interna de Docker frente al tráfico externo, confirmando que la gestión de copias de seguridad debe realizarse dentro del propio ecosistema del clúster.\n",
    "\n",
    "- **Saturación por Simetría (RF=4):** La configuración de replicación total (RF=4 en 4 nodos) fuerza una ocupación idéntica en todos los DataNodes. Esta rigidez anula la capacidad del sistema para gestionar el balanceo de carga o reasignar bloques, eliminando la flexibilidad operativa necesaria en entornos de producción.\n",
    "\n",
    "### 6.2) Recomendaciones\n",
    "\n",
    "- **Arquitectura de Almacenamiento (RF=3):** Se recomienda establecer el Factor de Replicación 3 como estándar. Esta configuración ofrece el equilibrio óptimo entre integridad (tolerancia a fallo simultáneo de 2 nodos) y eficiencia, conservando un 25% de capacidad de almacenamiento libre para operaciones temporales y balanceo de carga.\n",
    "\n",
    "- **Estrategia de Integridad Cíclica:** Para garantizar la calidad del dato sin comprometer el rendimiento del NameNode, se debe implementar el siguiente flujo operativo secuencial: **Ingesta → Auditoría (`fsck`) → Backup Diario.** Esto asegura que nunca se respalden datos corruptos.\n",
    "\n",
    "- **Política de Recuperación:** Se aconseja mantener la estrategia de **\"Full Snapshot\" diario** en el directorio `/backup`.\n",
    "    * *Justificación*: Dado el volumen actual del piloto (~1GB), el coste temporal es marginal (~14s), lo que proporciona una capa de seguridad imprescindible contra errores lógicos.\n",
    "    \n",
    "    * *Nota de Escalabilidad*: Aunque en entornos productivos masivos (TB/PB) sería obligatorio optar por estrategias incrementales (Incremental Backups) debido a los tiempos de transferencia, para el volumen actual la copia total ofrece la mejor relación entre simplicidad de restauración y seguridad.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
